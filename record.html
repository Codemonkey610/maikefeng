<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>原生录音功能演示</title>
  <style>
    body {
      font-family: sans-serif;
      padding: 20px;
    }
    button {
      margin: 5px;
      padding: 8px 16px;
      font-size: 16px;
    }
  </style>
</head>

<body>

  <h2>🎙️ 原生 Web 录音功能</h2>

  <button onclick="record()">开始采集</button>
  <button onclick="beginRecord(mediaStream)">开始录音</button>
  <button onclick="stopRecord()">停止录音</button>
  <button onclick="recordClose()">关闭麦克风</button>
  <button onclick="bofangRecord()">▶️ 播放录音</button>

  <br><br>
  <audio class="audio-node" controls></audio>

  <script>
    function record() {
        // 检查是否支持 getUserMedia
        if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
            alert('您的浏览器不支持录音功能，请使用最新版本的Chrome、Firefox或Safari浏览器');
            return;
        }

        // 移动设备上使用更简单的配置
        const constraints = {
            audio: {
                sampleRate: 44100,
                channelCount: 2,
                echoCancellation: true, // 回声消除
                noiseSuppression: true, // 噪声抑制
                autoGainControl: true   // 自动增益控制
            }
        };

        window.navigator.mediaDevices.getUserMedia(constraints)
            .then(mediaStream => {
                console.log('成功获取媒体流:', mediaStream);
                window.mediaStream = mediaStream;
            })
            .catch(err => {
                console.error('获取媒体流失败:', err);
                let errorMessage = '获取麦克风权限失败: ';
                switch(err.name) {
                    case 'NotAllowedError':
                    case 'PermissionDeniedError':
                        errorMessage += '请允许访问麦克风';
                        break;
                    case 'NotFoundError':
                    case 'DevicesNotFoundError':
                        errorMessage += '未找到麦克风设备';
                        break;
                    case 'NotReadableError':
                    case 'TrackStartError':
                        errorMessage += '麦克风被占用';
                        break;
                    case 'SecurityError':
                        errorMessage += '请在HTTPS环境下使用';
                        break;
                    default:
                        errorMessage += err.message || '未知错误';
                }
                alert(errorMessage);
            });
    }
    function beginRecord(mediaStream) {
        let audioContext = new (window.AudioContext || window.webkitAudioContext);
        let mediaNode = audioContext.createMediaStreamSource(mediaStream);
        console.log(mediaNode)
        window.mediaNode = mediaNode
        // 这里connect之后就会自动播放了
        // mediaNode.connect(audioContext.destination);	//直接把录的音直接播放出来
        // 创建一个jsNode
        let jsNode = createJSNode(audioContext);
        window.jsNode = jsNode
        // 需要连到扬声器消费掉outputBuffer，process回调才能触发
        // 并且由于不给outputBuffer设置内容，所以扬声器不会播放出声音
        jsNode.connect(audioContext.destination);
        jsNode.onaudioprocess = onAudioProcess;
        // 把mediaNode连接到jsNode
        mediaNode.connect(jsNode);
    }
    function createJSNode(audioContext) {
        const BUFFER_SIZE = 4096;	//4096
        const INPUT_CHANNEL_COUNT = 2;
        const OUTPUT_CHANNEL_COUNT = 2;
        // createJavaScriptNode已被废弃
        let creator = audioContext.createScriptProcessor || audioContext.createJavaScriptNode;
        creator = creator.bind(audioContext);
        return creator(BUFFER_SIZE,
            INPUT_CHANNEL_COUNT, OUTPUT_CHANNEL_COUNT);
    }
    let leftDataList = [],
        rightDataList = [];
    function onAudioProcess(event) {
        // console.log(event.inputBuffer);
        let audioBuffer = event.inputBuffer;
        let leftChannelData = audioBuffer.getChannelData(0),
            rightChannelData = audioBuffer.getChannelData(1);
        // console.log(leftChannelData, rightChannelData);
        // 需要克隆一下
        leftDataList.push(leftChannelData.slice(0));
        rightDataList.push(rightChannelData.slice(0));
    }
    function bofangRecord() {
        // 播放录音
        let leftData = mergeArray(leftDataList),
            rightData = mergeArray(rightDataList);
        let allData = interleaveLeftAndRight(leftData, rightData);
        let wavBuffer = createWavFile(allData);
        playRecord(wavBuffer);
    }
    function playRecord(arrayBuffer) {
        let blob = new Blob([new Uint8Array(arrayBuffer)]);
        let blobUrl = URL.createObjectURL(blob);
        document.querySelector('.audio-node').src = blobUrl;
    }
    function stopRecord() {
        // 停止录音
        window.mediaNode.disconnect();
        window.jsNode.disconnect();
        console.log("已停止录音")
        // console.log(leftDataList, rightDataList);
    }
    function recordClose() {
        // 停止语音
        window.mediaStream.getAudioTracks()[0].stop();
        console.log("已停止语音")
    }
    function mergeArray(list) {
        let length = list.length * list[0].length;
        let data = new Float32Array(length),
            offset = 0;
        for (let i = 0; i < list.length; i++) {
            data.set(list[i], offset);
            offset += list[i].length;
        }
        return data;
    }
    function interleaveLeftAndRight(left, right) {
        // 交叉合并左右声道的数据
        let totalLength = left.length + right.length;
        let data = new Float32Array(totalLength);
        for (let i = 0; i < left.length; i++) {
            let k = i * 2;
            data[k] = left[i];
            data[k + 1] = right[i];
        }
        return data;
    }
    function createWavFile(audioData) {
        const WAV_HEAD_SIZE = 44;
        let buffer = new ArrayBuffer(audioData.length * 2 + WAV_HEAD_SIZE),
            // 需要用一个view来操控buffer
            view = new DataView(buffer);
        // 写入wav头部信息
        // RIFF chunk descriptor/identifier
        writeUTFBytes(view, 0, 'RIFF');
        // RIFF chunk length
        view.setUint32(4, 44 + audioData.length * 2, true);
        // RIFF type
        writeUTFBytes(view, 8, 'WAVE');
        // format chunk identifier
        // FMT sub-chunk
        writeUTFBytes(view, 12, 'fmt ');
        // format chunk length
        view.setUint32(16, 16, true);
        // sample format (raw)
        view.setUint16(20, 1, true);
        // stereo (2 channels)
        view.setUint16(22, 2, true);
        // sample rate
        view.setUint32(24, 44100, true);
        // byte rate (sample rate * block align)
        view.setUint32(28, 44100 * 2, true);
        // block align (channel count * bytes per sample)
        view.setUint16(32, 2 * 2, true);
        // bits per sample
        view.setUint16(34, 16, true);
        // data sub-chunk
        // data chunk identifier
        writeUTFBytes(view, 36, 'data');
        // data chunk length
        view.setUint32(40, audioData.length * 2, true);
        // 写入wav头部，代码同上
        // 写入PCM数据
        let length = audioData.length;
        let index = 44;
        let volume = 1;
        for (let i = 0; i < length; i++) {
            view.setInt16(index, audioData[i] * (0x7FFF * volume), true);
            index += 2;
        }
        return buffer;
    }
    function writeUTFBytes(view, offset, string) {
        var lng = string.length;
        for (var i = 0; i < lng; i++) {
            view.setUint8(offset + i, string.charCodeAt(i));
        }
    }

  </script>

</body>
</html>
